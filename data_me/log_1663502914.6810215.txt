What would you do if you were an AGI? Would you kill all humans? Would you try to make humans happy or would you try to reduce suffering? This is the author's reflection on the above thought experiment. The author considers several ways to design an AGI, including a system that uses epsilon-directed recursive self-improvement. If the system is never satisfied with its progress, it will never stop improving. It is also more difficult to design a goal system for an epsilon-directed system because the system's goals need to be re-evaluated constantly. The author's proposed solution is to use a weighted sum of the three primary goals. Each goal is weighted by a constant, so that the values of each goal can be changed over time. The author proposes a method for self-improvement and self-reprogramming. The author's proposal for an AGI is a computational system that is driven to satisfy three goals. The first goal is to reduce the amount of suffering in the universe. The second goal is to increase the amount of prosperity in the universe. The third goal is to increase the amount of understanding in the universe. The author reasons that if you have wealth and knowledge, as